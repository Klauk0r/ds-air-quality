{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial exploratory analysis and generation of a baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Data cleaning and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data (train.csv will be the complete dataset for predictive modeling, ignore Test.csv for now)\n",
    "data=pd.read_csv(\"./data/Train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first five observations\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick examination shows that the dataset contains an ID column, a location column indicating the sensor location, 6 feature variables containing sensor data to be used for target prediction, as well as the target itself.\n",
    "\n",
    "The 6 feature variables are: temperature, precipitation, relative humidity, wind direction, wind speed, atmospheric pressure\n",
    "\n",
    "For each observation, the data for the feature variables reflects the raw sensor measurements collected hourly over a 5-day period. These measurement periods are stored as a string per observation / feature. They thus need to be unpacked into list. As can be seen from the data table, each observation period contains varying degrees of missings (NaN), which need to be managed in a proper way. These missings occur either in the form of 'nan' or spaces in the string.\n",
    "Also, it would be good to extract summary statistics from the different features for each recording period (e.g. mean, standard deviation, minimum, maximum) in order to engineer new features for the prediction.\n",
    "\n",
    "The target represents the amount of PM2.5 particles in ug/m^3 measured exactly 24h after the end of the recording period for the feature measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the sensor data for each feature from string type into a list of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to replace spaces in the string (i.e. missings) with NaN\n",
    "def replace_nan(x):\n",
    "    if x==\" \":\n",
    "        return np.nan\n",
    "    else :\n",
    "        return float(x)\n",
    "\n",
    "# define list of feature names\n",
    "features=[\"temp\",\"precip\",\"rel_humidity\",\"wind_dir\",\"wind_spd\",\"atmos_press\"]\n",
    "\n",
    "\n",
    "for feature in features : \n",
    "    # first replace every 'nan' in a cell with an empty space, split using comma, and then apply replace_nan function on every item\n",
    "    data[feature]=data[feature].apply(lambda x: [ replace_nan(X) for X in x.replace(\"nan\",\" \").split(\",\")])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, for every observation, each feature value is represented as a list of raw values over an hourly 5-day recording period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features engineering part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract summary statistics per observation period for every feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = data[['location', 'temp']]\n",
    "df_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp['percent_nan'] = data.temp.apply(lambda x: np.isnan(np.array(x)).sum()/len(x)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recording periods contain varying degrees of NaNs: compute percent NaN for each recording period and feature\n",
    "# function to compute the percentage of NaNs per recording period\n",
    "def compute_percent_nan(df, col_name):\n",
    "    df['percent_nan_'+col_name] = df[col_name].apply(lambda x: np.isnan(np.array(x)).sum()/len(x)*100)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate percentage of missings per recording period and feature and append to dataframe\n",
    "for col_name in tqdm(features):\n",
    "    data=compute_percent_nan(data,col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregation function extracting summary statistics from every recording period and appending it as a new column to a dataframe\n",
    "def aggregate_features(x,col_name):\n",
    "    x[\"max_\"+col_name]=x[col_name].apply(np.max)\n",
    "    x[\"min_\"+col_name]=x[col_name].apply(np.min)\n",
    "    x[\"mean_\"+col_name]=x[col_name].apply(np.mean)\n",
    "    x[\"std_\"+col_name]=x[col_name].apply(np.std)\n",
    "    x[\"var_\"+col_name]=x[col_name].apply(np.var)\n",
    "    x[\"median_\"+col_name]=x[col_name].apply(np.median)\n",
    "    x[\"ptp_\"+col_name]=x[col_name].apply(np.ptp)\n",
    "    return x  \n",
    "\n",
    "# function returning only non-Null values (helper for aggregation function)\n",
    "def remove_nan_values(x):\n",
    "    return [e for e in x if not math.isnan(e)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove NaNs from dataframe\n",
    "for col_name in tqdm(features):\n",
    "   data[col_name]=data[col_name].apply(remove_nan_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract summary statistics for each recording period and feature\n",
    "for col_name in tqdm(features):\n",
    "    data=aggregate_features(data,col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all single raw values from each feature and append them as new columns to the dataframe\n",
    "for x in range(121):\n",
    "    data[\"newtemp\"+ str(x)] = data.temp.str[x]\n",
    "    data[\"newprecip\"+ str(x)] = data.precip.str[x]\n",
    "    data[\"newrel_humidity\"+ str(x)] = data.rel_humidity.str[x]\n",
    "    data[\"newwind_dir\"+ str(x)] = data.wind_dir.str[x]\n",
    "    data[\"windspeed\"+ str(x)] = data.wind_spd.str[x]\n",
    "    data[\"atmospherepressure\"+ str(x)] = data.atmos_press.str[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop raw sensor data contained as list from the initial dataset\n",
    "data.drop(features,1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list dataframe columns for target and condensed features, including percent NaNs\n",
    "summary_columns = data.columns[:51]\n",
    "summary_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select data with summary statistics for the different features\n",
    "df_condensed = data[summary_columns]\n",
    "df_condensed.set_index('ID', drop=True, inplace=True)\n",
    "df_condensed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute basic summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_condensed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get descriptive statistics on the percentage of NaNs for the recording period\n",
    "df_condensed[['percent_nan_temp', 'percent_nan_precip',\n",
    "       'percent_nan_rel_humidity', 'percent_nan_wind_dir',\n",
    "       'percent_nan_wind_spd', 'percent_nan_atmos_press']].describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution and correlations of percent NaNs between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the overall distribution of percent NaNs depending on sensor location using histograms\n",
    "\n",
    "nan_columns = ['percent_nan_temp', 'percent_nan_precip','percent_nan_rel_humidity', 'percent_nan_wind_dir','percent_nan_wind_spd', \n",
    "        'percent_nan_atmos_press']\n",
    "\n",
    "fig = plt.figure(figsize=(15, 20))\n",
    "\n",
    "for i in range(1, len(nan_columns)+1): # start with i=1 (0th subplot is not possible)\n",
    "    ax = fig.add_subplot(4, 2, i) # arrange figure as rows = 6 x cols = 4 panel and add ith subplot\n",
    "    subplot = sns.histplot(x=nan_columns[i-1], hue='location', bins=20, data=df_condensed)\n",
    "    ax.set_xlabel(nan_columns[i-1])\n",
    "\n",
    "fig.tight_layout() # prevents subplots from overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot pairplot to examine feature-wise correlations in the percent NaNs\n",
    "sns.pairplot(df_condensed[['location', 'percent_nan_temp', 'percent_nan_precip','percent_nan_rel_humidity', 'percent_nan_wind_dir','percent_nan_wind_spd', \n",
    "        'percent_nan_atmos_press']], hue='location');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter dataframe for observations with percent NaN < 30% for all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter observations based on percent NaN and check again the data distribution of the target and summary features\n",
    "df_filtered = df_condensed[(df_condensed[nan_columns]<30).all(axis=1)]\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_reduc = (df_condensed.shape[0]-df_filtered.shape[0]) / df_condensed.shape[0]*100\n",
    "print(f'Percent reduction in dataset size after filtering: {round(percent_reduc,1)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data distribution for the summary features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get target and summary feature columns\n",
    "colnames = df_condensed.columns.to_list()[1::] # get columns w/o location column\n",
    "del colnames[1:8] # delete columns containing data on percent NaN\n",
    "colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histograms on the distribution of the other data\n",
    "fig = plt.figure(figsize=(15, 20))\n",
    "\n",
    "for i in range(1, len(colnames)+1): # start with i=1 (0th subplot is not possible)\n",
    "    ax = fig.add_subplot(9, 5, i) # arrange figure as rows = 6 x cols = 4 panel and add ith subplot\n",
    "    subplot = sns.histplot(x=colnames[i-1], hue='location', data=df_condensed)\n",
    "    ax.set_xlabel(colnames[i-1])\n",
    "\n",
    "fig.tight_layout() # prevents subplots from overlapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean differences in air pollution between locations for filtered and unfiltered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 5))\n",
    "plt.suptitle('Air pollution depending on sensor location')\n",
    "\n",
    "plot1 = sns.boxplot(data=df_condensed, x='location', y='target', ax=ax1, order=['A', 'B', 'C', 'D', 'E'])\n",
    "ax1.set_title('unfiltered')\n",
    "ax1.set_ylabel('PM2.5 (ug/m^3')\n",
    "ax1.set_xlabel('Sensor location')\n",
    "\n",
    "plot2 = sns.boxplot(data=df_filtered, x='location', y='target', ax=ax2, order=['A', 'B', 'C', 'D', 'E'])\n",
    "ax2.set_title('filtered')\n",
    "ax2.set_ylabel('PM2.5 (ug/m^3)')\n",
    "ax2.set_xlabel('Sensor location')\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations between mean values for features and target for unfiltered and filtered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 5))\n",
    "plt.suptitle('Relationship between mean temperature and PM2.5')\n",
    "\n",
    "plot1 = sns.scatterplot(data=df_condensed, x='mean_temp', y='target', hue='location', ax=ax1, hue_order=['A', 'B', 'C', 'D', 'E'])\n",
    "plot1.set_title('unfiltered')\n",
    "\n",
    "plot2 = sns.scatterplot(data=df_filtered, x='mean_temp', y='target', hue='location', ax=ax2, hue_order=['A', 'B', 'C', 'D', 'E'])\n",
    "plot2.set_title('filtered')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 5))\n",
    "plt.suptitle('Relationship between mean precipitation and PM2.5')\n",
    "\n",
    "plot1 = sns.scatterplot(data=df_condensed, x='mean_precip', y='target', hue='location', ax=ax1, hue_order=['A', 'B', 'C', 'D', 'E'])\n",
    "plot1.set_title('unfiltered')\n",
    "\n",
    "plot2 = sns.scatterplot(data=df_filtered, x='mean_precip', y='target', hue='location', ax=ax2, hue_order=['A', 'B', 'C', 'D', 'E'])\n",
    "plot2.set_title('filtered')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 5))\n",
    "plt.suptitle('Relationship between mean humidity and PM2.5')\n",
    "\n",
    "plot1 = sns.scatterplot(data=df_condensed, x='mean_rel_humidity', y='target', hue='location', ax=ax1, hue_order=['A', 'B', 'C', 'D', 'E'])\n",
    "plot1.set_title('unfiltered')\n",
    "\n",
    "plot2 = sns.scatterplot(data=df_filtered, x='mean_rel_humidity', y='target', hue='location', ax=ax2, hue_order=['A', 'B', 'C', 'D', 'E'])\n",
    "plot2.set_title('filtered')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 5))\n",
    "plt.suptitle('Relationship between mean wind direction and PM2.5')\n",
    "\n",
    "plot1 = sns.scatterplot(data=df_condensed, x='mean_wind_dir', y='target', hue='location', ax=ax1, hue_order=['A', 'B', 'C', 'D', 'E'])\n",
    "plot1.set_title('unfiltered')\n",
    "\n",
    "plot2 = sns.scatterplot(data=df_filtered, x='mean_wind_dir', y='target', hue='location', ax=ax2, hue_order=['A', 'B', 'C', 'D', 'E'])\n",
    "plot2.set_title('filtered')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 5))\n",
    "plt.suptitle('Relationship between mean wind speed and PM2.5')\n",
    "\n",
    "plot1 = sns.scatterplot(data=df_condensed, x='mean_wind_spd', y='target', hue='location', ax=ax1, hue_order=['A', 'B', 'C', 'D', 'E'])\n",
    "plot1.set_title('unfiltered')\n",
    "\n",
    "plot2 = sns.scatterplot(data=df_filtered, x='mean_wind_spd', y='target', hue='location', ax=ax2, hue_order=['A', 'B', 'C', 'D', 'E'])\n",
    "plot2.set_title('filtered')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 5))\n",
    "plt.suptitle('Relationship between mean atmospheric pressure and PM2.5')\n",
    "\n",
    "plot1 = sns.scatterplot(data=df_condensed, x='mean_atmos_press', y='target', hue='location', ax=ax1, hue_order=['A', 'B', 'C', 'D', 'E'])\n",
    "plot1.set_title('unfiltered')\n",
    "\n",
    "plot2 = sns.scatterplot(data=df_filtered, x='mean_atmos_press', y='target', hue='location', ax=ax2, hue_order=['A', 'B', 'C', 'D', 'E'])\n",
    "plot2.set_title('filtered')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the basemodel\n",
    "We compute a baseline model, predicting the target using the mean values of the different features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries and metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get target and features\n",
    "X = df_condensed[['location', 'mean_temp', 'mean_precip', 'mean_rel_humidity', 'mean_wind_dir', 'mean_wind_spd', 'mean_atmos_press']]\n",
    "y = df_condensed.target\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform train test split, stratified by location in order to ensure that locations are balanced between training and test set\n",
    "rseed = 42\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=X['location'], random_state=rseed)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale features using z-transformation\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit_transform training data, drop location column since it will not be used for prediction\n",
    "X_train_scaled = scaler.fit_transform(X_train.drop('location', axis=1))\n",
    "# aply transform to test data\n",
    "X_test_scaled = scaler.transform(X_test.drop('location', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_scaled.shape)\n",
    "print(X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if transformation worked\n",
    "\n",
    "# calculate mean\n",
    "print('Means')\n",
    "for i in range(X_train_scaled.shape[1]):\n",
    "    print(X_train_scaled[:, i].mean().round(2))\n",
    "\n",
    "print('\\nStandard deviations:')\n",
    "# calculate standard deviation\n",
    "for i in range(X_train_scaled.shape[1]):\n",
    "    print(X_train_scaled[:, i].std().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "linreg = LinearRegression()\n",
    "\n",
    "linreg.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get beta coefficients\n",
    "linreg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict new cases\n",
    "y_pred = linreg.predict(X_test_scaled)\n",
    "\n",
    "# show first 10 predictions\n",
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model accurray\n",
    "\n",
    "rmse_linreg = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2_linreg = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'RMSE on testset: {round(rmse_linreg,2)}')\n",
    "print(f'Coefficient of determination on testset: {round(r2_linreg,2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate baseline model only for location B \n",
    "We re-perform our linear regression only using sensor data for location B, because the data for this location seem to be most representative of the whole data, without risking our model training being confounded by location-specific data due to the hierarchical data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract data only for location B\n",
    "df_locB = df_condensed[df_condensed.location == 'B']\n",
    "df_locB.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_locB.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get target and features\n",
    "X_locB = df_locB[['location', 'mean_temp', 'mean_precip', 'mean_rel_humidity', 'mean_wind_dir', 'mean_wind_spd', 'mean_atmos_press']]\n",
    "y_locB = df_locB.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform train test split, stratification not needed anymore\n",
    "rseed = 42\n",
    "\n",
    "X_train_locB, X_test_locB, y_train_locB, y_test_locB = train_test_split(X_locB, y_locB, test_size=0.3, random_state=rseed)\n",
    "\n",
    "print(X_train_locB.shape)\n",
    "print(X_test_locB.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale features using z-transformation\n",
    "scaler_locB = StandardScaler()\n",
    "\n",
    "# fit_transform training data, drop location column since it will not be used for prediction\n",
    "X_train_scaled_locB = scaler.fit_transform(X_train_locB.drop('location', axis=1))\n",
    "# aply transform to test data\n",
    "X_test_scaled_locB = scaler.transform(X_test_locB.drop('location', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "linreg_locB = LinearRegression()\n",
    "\n",
    "linreg_locB.fit(X_train_scaled_locB, y_train_locB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get beta coefficients\n",
    "linreg_locB.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict new cases\n",
    "y_pred_locB = linreg.predict(X_test_scaled_locB)\n",
    "\n",
    "# show first 10 predictions\n",
    "y_pred_locB[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model accurray\n",
    "\n",
    "rmse_linreg_locB = mean_squared_error(y_test_locB, y_pred_locB, squared=False)\n",
    "r2_linreg_locB = r2_score(y_test_locB, y_pred_locB)\n",
    "\n",
    "print(f'RMSE on testset: {round(rmse_linreg_locB,2)}')\n",
    "print(f'Coefficient of determination on testset: {round(r2_linreg_locB,2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate baseline model with filtered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get target and features\n",
    "X_filtered = df_filtered[['location', 'mean_temp', 'mean_precip', 'mean_rel_humidity', 'mean_wind_dir', 'mean_wind_spd', 'mean_atmos_press']]\n",
    "y_filtered = df_filtered.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform train test split, stratify according to sensor location\n",
    "rseed = 42\n",
    "\n",
    "X_train_f, X_test_f, y_train_f, y_test_f = train_test_split(X_filtered, y_filtered, test_size=0.3, \n",
    "                                            stratify=X_filtered['location'], random_state=rseed)\n",
    "\n",
    "print(X_train_f.shape)\n",
    "print(X_test_f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale features using z-transformation\n",
    "scaler_f = StandardScaler()\n",
    "\n",
    "# fit_transform training data, drop location column since it will not be used for prediction\n",
    "X_train_scaled_f = scaler.fit_transform(X_train_f.drop('location', axis=1))\n",
    "# aply transform to test data\n",
    "X_test_scaled_f = scaler.transform(X_test_f.drop('location', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "linreg_f = LinearRegression()\n",
    "\n",
    "linreg_f.fit(X_train_scaled_f, y_train_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get beta coefficients\n",
    "linreg_f.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict new cases\n",
    "y_pred_f = linreg.predict(X_test_scaled_f)\n",
    "\n",
    "# show first 10 predictions\n",
    "y_pred_f[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model accurray\n",
    "\n",
    "rmse_linreg_f = mean_squared_error(y_test_f, y_pred_f, squared=False)\n",
    "r2_linreg_f = r2_score(y_test_f, y_pred_f)\n",
    "\n",
    "print(f'RMSE on testset: {round(rmse_linreg_f,2)}')\n",
    "print(f'Coefficient of determination on testset: {round(r2_linreg_f,2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual plot for baseline model trained on filtered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate residuals\n",
    "residual = y_test_f - y_pred_f\n",
    "\n",
    "# compute mean of residuals\n",
    "np.mean(residual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, on average, our baseline model rather seems to underestimate the air pollution, as indicated by an average residual error > 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=y_pred_f, y=residual, hue=X_test_f['location'])\n",
    "plt.xlabel('y_pred')\n",
    "plt.ylabel('residual')\n",
    "plt.title('Residual plot');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=X_test_f.mean_temp, y=residual, hue=X_test_f['location'])\n",
    "plt.xlabel('mean temperature')\n",
    "plt.ylabel('residual')\n",
    "plt.title('Residual plot');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=X_test_f.mean_precip, y=residual, hue=X_test_f['location'])\n",
    "plt.xlabel('mean precipitation')\n",
    "plt.ylabel('residual')\n",
    "plt.title('Residual plot');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=X_test_f.mean_rel_humidity, y=residual, hue=X_test_f['location'])\n",
    "plt.xlabel('mean relative humidity')\n",
    "plt.ylabel('residual')\n",
    "plt.title('Residual plot');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=X_test_f.mean_wind_dir, y=residual, hue=X_test_f['location'])\n",
    "plt.xlabel('mean wind direction')\n",
    "plt.ylabel('residual')\n",
    "plt.title('Residual plot');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=X_test_f.mean_wind_spd, y=residual, hue=X_test_f['location'])\n",
    "plt.xlabel('mean wind speed')\n",
    "plt.ylabel('residual')\n",
    "plt.title('Residual plot');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=X_test_f.mean_atmos_press, y=residual, hue=X_test_f['location'])\n",
    "plt.xlabel('mean atmospheric pressure')\n",
    "plt.ylabel('residual')\n",
    "plt.title('Residual plot');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "The data has a different hierarchical, i.e. nested structure, with measurements being nested within defined sensor locations. This makes the dataset difficult to handle.\n",
    "\n",
    "\n",
    "Each observation consists of an 5-day recording period, where sensor data is recorded in an hourly fashion. We extracted summary statistics from those recording periods for each feature, using those statistics as new features to predict air pollution 24 h later, measured as the amount of PM2.5 particles in ug/m^3.\n",
    "\n",
    "\n",
    "A baseline model using a simple linear regression model using the mean values in the features per recording period to predict air pollution yielded almost no predictive power, as reflected in a **R2==0.05**. Calculating this baseline model only for a single sensor location (location B) also produced no usable prediction power. Notably, training the baseline model on filtered data resulted in an **R2==0.09**. However, it should be stressed that different train-test splits were performed to get training and test sets for the unfiltered and filtered data.\n",
    "\n",
    "However, the recording periods showing varying degrees of NaNs. An exploratory analysis on the distribution and correlation patterns of the percent NaNs shows that there are some recording periods with vary high percent NaNs (>30%). The percent NaN is usually highly correlated between the different features, thus affecting the whole data collection within a recording period and therefore probably reflecting general sensor malfunctioning. However, there are also some features showing more specific occurrences of NaNs that seem to be additionally associated with the sensor location.\n",
    "\n",
    "It would be important to define a threshold for the percent NaN to consider an observation reliable enough to extract summary statistics. This threshold can then be used as an exclusion criterion to filter the observations.\n",
    "\n",
    "**Possible steps to improve prediction accuracy**\n",
    "1) Log-transform the target variable (seems to show a skewed distribution)\n",
    "2) Standardize features within each sensor location to avoid cofounds by variables specific to sensor locations\n",
    "3) Perform hierarchical linear modeling\n",
    "4) Test different sets of features and/or different types of ML algorithms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "a74321acea7b933ac2b40f4839998b763c182e4cba40b24760128a90b96880fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
